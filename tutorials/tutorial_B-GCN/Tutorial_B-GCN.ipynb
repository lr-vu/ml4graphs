{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "23c32d24-ccd3-4a73-8609-72c92d5847b0",
   "metadata": {},
   "source": [
    "# Machine Learning for Graphs - Tutorial B: The Graph Convolutional Network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccb6f7c6-8d41-4cc9-835b-4bd0180be7f6",
   "metadata": {},
   "source": [
    "Fill in your names and group number here:\n",
    "\n",
    "**NAME STUDENT A :**\n",
    "\n",
    "**NAME STUDENT B :**\n",
    "\n",
    "**GROUP NUMBER :**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4eaa3c8-1ef9-4a70-b834-3aaf03e1051d",
   "metadata": {},
   "source": [
    "Implementing a machine learning experiment with graph data is an important skill that you will learn as part of this course. This hands-on tutorial will help you develop this skill, as well as help you familiarize yourself with many of the steps and techniques that you will likely need to use for your final project.\n",
    "\n",
    "Representation learning is the task of learning sensible representations for your samples given some downstream task. On graphs, representation learning is commonly used to learn vector representations of the nodes. These node representations are often called *embeddings vectors* or just *embeddings*. Graph Neural Networks (GNN) are ideal for learning node embeddings, since the identity of a node is a function of its neighbourhood (up to depth *d*) and since GNNs learn internal node representations by applying an aggregation operator on exactly this neighbourhood. Different models with various choices of aggregation operator have been introduced over the past couple of years, with the *convolutional* and *attention* operators being the more popular choices.\n",
    "\n",
    "For this tutorial, you are asked to implement the original *Graph Convolutional Network* (GCN) and to replicate some of the classification experiments from the [paper](https://arxiv.org/abs/1609.02907) that introduced it [1]. To help you on your way, we have already prepared this Python Notebook.\n",
    "\n",
    "You are asked to team up with another student and to work together on this tutorial. Please register your team by creating a new group and by adding both members.\n",
    "\n",
    "    [1] Kipf, T. N., & Welling, M. Semi-supervised Classification With Graph Convolutional Networks (2017).\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "293cfcff-c494-4866-b042-2d575793e833",
   "metadata": {},
   "source": [
    "## NumPy and PyTorch\n",
    "\n",
    "In this course we will make use of the [NumPy](https://numpy.org) package for working with vector data, and the [PyTorch](https://pytorch.org) machine learning package. Both of these are probably already installed in your environment as part of the first tutorial (Numpy as a dependency of PyTorch) but if this is not the case then running the following cell will install these packages for you.\n",
    "\n",
    "**Run the cell below to install the NumPy and PyTorch packages in your Python environment**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bb3657a-3cf8-4bbb-9290-1f7d3df6867e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install numpy torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7057c299-0ef8-4655-9184-5f2f71045199",
   "metadata": {},
   "source": [
    "**Run the cells below to import the necessay packages and to set a manual seed**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dcb06c7-0b58-439b-83a5-21293e6bb4c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "seed = 42  # for reproducability\n",
    "torch.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "148ddba6",
   "metadata": {},
   "source": [
    "## Data Preparation\n",
    "\n",
    "In the previous tutorial we used the *RDFlib* package to import the dataset. That the dataset was encoded using an open standard made this possible. This is not always the case, however: it is very common to come accross graph datasets that use an arbitrary encoding. In the case of the *Cora* dataset loaded below, the graph has been stored in two parts: the first a set of integer-encoded edges $[i, j]$, with $i$ and $j$ the indices of the nodes, and the second as a set of *n*-hot encoded node representations. Being a citation graph, the edges convey who cites who, whereas the node vectors $e_i$ represent a sparse bag-of-words with vocabulary $\\Omega$ for which holds that $e_i[j] = 1$ if word $\\Omega[j]$ occurs in the document and $0$ otherwise.\n",
    "\n",
    "To import the Cora dataset we first process the raw files using NumPy and cast the generated arrays to the correct datatypes. Next, we generate a node-to-integer map and reindex the edges to ensure that their node identifiers match those of the nodes.\n",
    "\n",
    "**Run the following cells to import and process the data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03a1481a",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = './data/'\n",
    "\n",
    "data = np.genfromtxt(path + \"cora.content\", dtype = str)\n",
    "edges = np.genfromtxt(path + \"cora.cites\", dtype = int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e37c5d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# these have the same order\n",
    "features = data[:, 1:-1].astype(int)\n",
    "labels = data[:, -1]\n",
    "nodes = data[:, 0].astype(int)\n",
    "\n",
    "n2i = {n:i for i,n in enumerate(nodes)}\n",
    "edges_reindexed = np.array([[n2i[source], n2i[target]] for source, target in edges])\n",
    "\n",
    "num_nodes = len(nodes)\n",
    "num_edges = len(edges)\n",
    "num_features = len(features[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83bc818a-8814-4e55-8c1d-633dc643a7c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# inspect the data\n",
    "print(f\"Number of nodes: {num_nodes}\")\n",
    "print(f\"Number of edges: {num_edges}\")\n",
    "print(f\"Number of features: {num_features}\\n\")\n",
    "\n",
    "for i in range(5):\n",
    "    print(f\"Node ID: {nodes[i]}\")\n",
    "    print(f\"Node features: {features[i]}\")\n",
    "    print(f\"Node label: {labels[i]}\\n\")\n",
    "    \n",
    "print(f\"Edges: \\n{edges_reindexed[:5]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5d06ddb",
   "metadata": {},
   "source": [
    "## Task 1: Vectorizing the graph"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4925195e-b8e5-4429-9768-564fe63f8a18",
   "metadata": {},
   "source": [
    "Since graph neural networks aggregate the information from the neighbourhoods of nodes, they need to know which nodes are adjacent to which other nodes. Because the information from those neighbours must also be aggregated from *their* neighbourhoods, these models thus need a relatively large amount of information about the structure of a graph. This information comes in the form of an *adjacency matrix* $A$, such that $A[i,j] = 1$ if there exists a link between nodes $i$ and $j$, and $0$ otherwise.\n",
    "\n",
    "Of course, the adjacency matrix only tells the model which nodes to aggregate. To also know *what* to aggregate, we need another matrix which uniquely identifies each node. This matrix is often called the *node feature matrix* $X$. If our nodes comes with one or more attributes, or *features*, then we can fill up this matrix with the corresponding values. This is commonly done with *multimodal learning*. More often, however, it is easier to just ignore the node features (if any), and to let $X$ equal the identity matrix $I$ such that $X[i,j] = 1$ iff $i = j$ and $0$ otherwise.\n",
    "\n",
    "Finally, since the downstream task is *node classification*, we need a vector representation, the *target vector* $y$, for the class labels that are used to compute the loss and accuracy scores. Since we need to calculate the gradients during this step, we need a numerical encoding for the labels. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a66ea26",
   "metadata": {},
   "source": [
    "### Task 1a: Creating a feature matrix\n",
    "\n",
    "Write a procedure to generate a node feature matrix that maps each node to its respective feature vector. The result should be a *sparse* float tensor `X`, such that `X[i]` refers to the feature vector of node `i`. Since the Cora dataset comes with integer-encoded node features (the bag-of-words) there is no need to generate an indentity matrix. Remember that the whole set of features is stored in variable `features`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b8aac48",
   "metadata": {},
   "outputs": [],
   "source": [
    "...\n",
    "X = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0a77ca6",
   "metadata": {},
   "source": [
    "Run the following code to check your feature matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f93befc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check your feature matrix\n",
    "X.to_dense()[:10,:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08678179",
   "metadata": {},
   "source": [
    "### Task 1b: Create an adjacency matrix\n",
    "\n",
    "Write a procedure to generate the adjacency matrix for the Cora graph. The result should be a *sparse* float tensor `A`, such that `A[i,j]` equals `1` if there exists an edge between nodes `i` and `j`, and `0` otherwise. Be aware that the GCN requires all nodes to have a reflexive edge (loops) which ensures that the nodes remember their previous state when updating.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f6db863",
   "metadata": {},
   "outputs": [],
   "source": [
    "...\n",
    "A = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3497674",
   "metadata": {},
   "source": [
    "Run the following code to check your adjacency matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b47de91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check your adjacency matrix by using the sum as proxy\n",
    "print(f\"The number of connections, {int(A.sum())}, must equal the number of edges, {num_edges},\" \n",
    "      f\" plus the number of nodes, {num_nodes}\")\n",
    "A.to_dense()[:10,:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96879c15",
   "metadata": {},
   "source": [
    "### Task 1c: Create the target vector\n",
    "\n",
    "Write a procedure to generate the target vector with integer-encoded class labels. The result should be a `long` vector `y_true`, such that `y_true[i]` holds the target label of node `i`. Note that, with PyTorch, different loss functions require differently formatted target vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8d9a37f",
   "metadata": {},
   "outputs": [],
   "source": [
    "...\n",
    "\n",
    "y_true = ...\n",
    "\n",
    "num_labels = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "966e6b00",
   "metadata": {},
   "source": [
    "Run the following code to check your target vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "720aa91e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'number of unique labels: {num_labels}\\n')\n",
    "\n",
    "print(f'y: {y_true[:10]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f8ff568",
   "metadata": {},
   "source": [
    "## Task 2: Partition the dataset\n",
    "\n",
    "To properly perform our experiments we first need to partition our data into a _train_ and _test_ split. These splits are used to train and test our model, respectively, and must be disjoint to avoid information leakage. Ideally, we would als create a _validation_ split to use for model selection and/or hyperparameter optimization, but we dispense with that for now.\n",
    "\n",
    "Create a procedure to create a train and test split with a ratio of 4 to 1. The result should be two vectors, `train_idx` and `test_idx`, that contain indices that point to the actual data (a _mask_) that are randomly drawn from the set of all indices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a09368e",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_train = ...\n",
    "num_test = ...\n",
    "\n",
    "# use mask\n",
    "...\n",
    "train_idx = ...\n",
    "test_idx = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6400fe68",
   "metadata": {},
   "source": [
    "Run the following code to check your partitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c5bde0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"number of training samples: {num_train}\")\n",
    "print(f\"number of testing samples: {num_test}\")\n",
    "\n",
    "print(f\"\\ntrain indices:\\n{train_idx[:5]}\")\n",
    "print(f\"\\ntest indices:\\n{test_idx[:5]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62b03bdd-2e7e-4abd-9e59-1c93fc1f8665",
   "metadata": {},
   "source": [
    "## The Graph Convolutional\n",
    "\n",
    "The *Graph Convolutional Network* (GCN) is arguably the first major breakthrough in GNN development. Developed in 2017, the GCN introduces the idea of the *spectral graph convolution*, which, analogues to its visual counterpart, aggregates the information surrounding an object. In the case of *Convolutional Neural Networks* (CNN), these objects are pixels, whereas with the GCN these are nodes. This comparison becomes evident when you consider images as regular (grid-shaped) graphs with pixel as nodes.\n",
    "\n",
    "The GCN is defined as a network with one or more *Graph Convolution* layers. Each of these layers applies the convolution operator to its input, and is defined as \n",
    "\n",
    "$$ H^{l+1} = \\sigma(\\tilde{D}^{- \\frac{1}{2}} \\tilde{A} \\tilde{D}^{- \\frac{1}{2}} H^l W^l) $$\n",
    "\n",
    "where $\\tilde{A}$ is the adjacency matrix with reflexive edges, $\\tilde{D}$ the degree matrix derived from $\\tilde{A}$, $H^l$ the internal node representations of layer $l$, $W^l$ the weight matrix of layer $l$, and $\\sigma$ a nonlinearity like $ReLU$. Note that the initial node representation matrix $H^0 = X$.\n",
    "\n",
    "In the experiments that we are reproducing the GCN is used for the task of node classification. For this purpose, the GCN is given two graph convolution layers, but with the nonlinearity of the last layer replaced by a softmax function:\n",
    "\n",
    "$$ y = softmax(\\hat{A}~\\sigma(\\hat{A} X W^0)~W^1) $$\n",
    "\n",
    "with $\\hat{A} = \\tilde{D}^{- \\frac{1}{2}} \\tilde{A} \\tilde{D}^{- \\frac{1}{2}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "281a4d5d",
   "metadata": {},
   "source": [
    "### Task 3a: Implement the Graph Convolution\n",
    "\n",
    "Implement the graph convolution layer as a subclass of PyTorch `nn.Module`. Concretely, you must implement the `__init__` and `forward` functions. Ensure that the computation supports sparse tensors, and that the input and output dimensions can be set on initialisation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b50a9a8-2903-494b-8c74-7922fde18ec9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphConvolutionLayer(nn.Module):\n",
    "    \"\"\"\n",
    "    A single Graph Convolution Layer\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, ...):\n",
    "        super().__init__()\n",
    "        \n",
    "        # your code here\n",
    "\n",
    "    def forward(self, ...) -> torch.Tensor:\n",
    "        # your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdb94fa1",
   "metadata": {},
   "source": [
    "Run the following cell to initialize and test your implementation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07028e52",
   "metadata": {},
   "outputs": [],
   "source": [
    "conv = GraphConvolutionLayer(...)\n",
    "conv(...)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae08cf12-16ae-408d-9713-3af4d72afa7e",
   "metadata": {},
   "source": [
    "### Task 3b: Implement the Graph Convolutional Model\n",
    "\n",
    "Implement the GCN as specified in the paper [1]. Concretely, implement a two-layer GCN with a ReLU activation function and dropout after the first layer, and with a softmax layer after the second."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5bb694b-1728-4696-9b8e-eb93c5f2fe7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GCN(nn.Module):\n",
    "    def __init__(self, ...):\n",
    "        super().__init__()\n",
    "\n",
    "        # your code here\n",
    "\n",
    "    def forward(self, ...) -> torch.Tensor:\n",
    "        # your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "640ccfc8-6f4a-4fc9-911f-0635e3e5e193",
   "metadata": {},
   "source": [
    "Run the following cell to initialize and test your implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6abd3180-b720-4b5d-9d19-dbdfa6953c11",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GCN(...)\n",
    "\n",
    "y_pred = model(...)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6a76d1a",
   "metadata": {},
   "source": [
    "## Training and testing\n",
    "\n",
    "In normal circumstances the GCN updates its internal representation for all nodes in the graph after each pass. In other words, the GCN operates on the entire graph at once, rather than on just the training, test, or validation set. Since these sets are disjoint, it necessarily means that only part of the class labels are available each time. This is called *semi-supervised learning*. Because the model sees the entire graph each pass, it still outputs predictions for all the nodes. However, by just calculating the loss and accuracy on a specific split, we ensure that only the error on the nodes in that split is backpropagated."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f16fa87d",
   "metadata": {},
   "source": [
    "### Task 4: Implementing evaluation metrics\n",
    "\n",
    "Write a procedure to calculate the loss *and* a procedure to calculate the accuracy. Assume that we have a tensor with true labels, `y_true`, and a tensor with predicted labels, `y_pred`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0922ab6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_function = nn.NLLLoss()\n",
    "\n",
    "def compute_accuracy(...) -> float:\n",
    "    # your code here\n",
    "\n",
    "def compute_loss(...) -> torch.Tensor:\n",
    "    # your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48a7a5e7-cdf3-4960-af2f-9a96d9a9fc0a",
   "metadata": {},
   "source": [
    "Run the following cell to test your code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1e2dbfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_labels = ...\n",
    "print(f'Predicted labels: {y_pred_labels[:10]}')\n",
    "\n",
    "acc = compute_accuracy(...)\n",
    "print(f'Accuracy: {acc:.3f}')\n",
    "\n",
    "loss = compute_loss(...)\n",
    "print(f'Loss: {loss:.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad95f107-2f27-47f2-a827-96337ab6aeda",
   "metadata": {},
   "source": [
    "### Task 5a: Implement the training loop\n",
    "\n",
    "Write a procedure to train the model. Specifically, create a loop that passes the entire graph through the model every epoch, while computing the loss and accuracy on just the training set. Use the Adam optimizer and the negative log likelihood loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53423cec-086b-4110-a8b2-206e7f02c9fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set hyperparameters\n",
    "learning_rate = 0.01\n",
    "num_epoch = 200\n",
    "\n",
    "# set optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(),\n",
    "                             lr = learning_rate)\n",
    "\n",
    "for epoch in range(1, num_epoch+1):\n",
    "    print(f'Epoch {epoch:3d} - ', end='')\n",
    "\n",
    "    # allow model parameters to be learned   \n",
    "    model.train()         \n",
    "\n",
    "    # your code here\n",
    "\n",
    "    # Zero gradients, perform a backward pass, and update the weights.\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    loss = float(loss)  # release memory of computation graph\n",
    "\n",
    "    print(f'loss: {loss:0.4f}\\tacc: {acc:0.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cdc2246-8bd8-4d5b-aca1-89d2a0a65211",
   "metadata": {},
   "source": [
    "### Task 5b: Implement the test procedure\n",
    "\n",
    "Write a procedure to test the now-trained model. Ensure that the weights of your model are frozen during testing, and that the loss and accuracy scores are calculated on just the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4156df0f-3662-471c-90e8-1eb30d9d24e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# freeze model parameters for evaluation\n",
    "model.eval()\n",
    "\n",
    "# your code here\n",
    "\n",
    "loss = float(loss)  # release memory of computation graph\n",
    "\n",
    "print(f'test loss: {loss:0.4f}\\ntest acc: {acc:0.4f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml4graphs",
   "language": "python",
   "name": "ml4graphs"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
