{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "23c32d24-ccd3-4a73-8609-72c92d5847b0",
   "metadata": {},
   "source": [
    "# Machine Learning for Graphs - Tutorial A: Triple Scoring with DistMult"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba6264ef-2f16-4438-9250-00fc5cffe95f",
   "metadata": {},
   "source": [
    "*Fill in your names and group number here:*\n",
    "\n",
    "**NAME STUDENT A** :  \n",
    "\n",
    "**NAME STUDENT B** :  \n",
    "\n",
    "**GROUP NUMBER** :  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "183fbe0e-b698-4d50-92ad-a525a5d2e85c",
   "metadata": {},
   "source": [
    "Implementing a machine learning experiment with graph data is an important skill that you will learn as part of this course. This hands-on tutorial will help you develop this skill, as well as help you familiarize yourself with many of the steps and techniques that you will likely need to use for your final project.\n",
    "\n",
    "The oldest and arguably most well-studied machine learning task for graphs is that of *link prediction*. The goal of this task is, as the name implies, to predict the existence of edges in a graph. Since real-world graphs often have large gaps in the information they encode, represented by missing links, and because this task has the potential of predicting these missing links, link prediction is also known as *knowledge base completion*.\n",
    "\n",
    "For this tutorial, you are asked to implement a link prediction model. Once correctly implemented, you are to train and test the model on a graph-based dataset that you have prepared during the first half of the tutorial. To help you on your way, we have already prepared this Python Notebook.\n",
    "\n",
    "You are asked to team up with another student and to work together on this tutorial. Please register your team by creating a new group and by adding both members.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b9f7b4e-777e-4691-9fa8-c7ca102a2b6c",
   "metadata": {},
   "source": [
    "## Working with Graphs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "293cfcff-c494-4866-b042-2d575793e833",
   "metadata": {},
   "source": [
    "Most major programming languages do not support graph data out of the box. Therefore, before we can continue, we first need to download and install the [RDFlib package](https://rdflib.readthedocs.io/en/stable/), which extends Python with an interface to graphs that are encoded using the *Resource Description Format* (RDF). The RDF data model is a W3C open standard to encode knowledge, information, and data in graph form, and is a common preferred choice for creating *knowledge graphs*. Rather than using the common prefix notation for links, i.e., `relation(head, tail)`, RDF knowledge graphs use the *in-fix* notation: `(head, relation, tail)`. Because of this, the links in a knowledge graph are often also called *triples*.\n",
    "\n",
    "**Run the cells below to install the RDFlib package in your Python environment and to import and inspect the graph**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bb3657a-3cf8-4bbb-9290-1f7d3df6867e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# install the rdflib package\n",
    "%pip install rdflib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de621539-e0a0-4484-ae40-2ce0a47f5e7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip\n",
    "import rdflib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54b8c604-294c-4025-b9a2-27ea8b1b1ab1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the data from disk\n",
    "\n",
    "g_train = rdflib.Graph()\n",
    "g_test = rdflib.Graph()\n",
    "\n",
    "path = './data/'\n",
    "with gzip.open(path + f'wn18rr_train.nt.gz', 'r') as gf:\n",
    "    g_train.parse(data = gf.read(), format = 'nt')\n",
    "with gzip.open(path + f'wn18rr_test.nt.gz', 'r') as gf:\n",
    "    g_test.parse(data = gf.read(), format = 'nt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83bc818a-8814-4e55-8c1d-633dc643a7c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test whether reading the data was successful by printing 10 links from the train and test set each\n",
    "\n",
    "print('TRAIN:', end='\\n')\n",
    "for h,r,t in list(g_train)[:10]:\n",
    "    print(f'{h} {r} {t}', end='\\n')\n",
    "\n",
    "print('TEST:', end='\\n')\n",
    "for h,r,t in list(g_test)[:10]:\n",
    "    print(f'{h} {r} {t}', end='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4925195e-b8e5-4429-9768-564fe63f8a18",
   "metadata": {},
   "source": [
    "### PyTorch\n",
    "\n",
    "In this course we will make use of the [*PyTorch*](https://pytorch.org/) machine learning package, which provides a large array of convenient functions and tools to implement and run machine learning experiments. The core data structure in PyTorch is the *tensor*. Taking a broad definition of the tensor, it is used in PyTorch to store almost everything from scalars and vectors to matrices and higher-order tensors.\n",
    "\n",
    "Much of your time will be spend working with this package.\n",
    "\n",
    "**Run the cells below to install the PyTorch package in your Python environment and to set a manual seed**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80bb12a9-b396-41c4-866c-1bcc164409b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# install pytorch\n",
    "%pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b6e7d8e-8ed3-420f-a849-84c49e678230",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "seed = 42\n",
    "torch.manual_seed(seed)  # allow for reproducability"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f30e4762-84ee-4bc7-909b-08bb9b1bd8ef",
   "metadata": {},
   "source": [
    "## Data Preparation\n",
    "\n",
    "Knowledge graphs are an extension of *labeled digraphs* and, as such, have labeled arcs and nodes. In case of the arcs, these labels represent relationship types, which typically occur more than once. In other words, arcs with the same label convey the same relationship. The situation is more complex for the nodes: some of the nodes represent *entities* (unique 'things', tangible or otherwise) which are labeled using the *Universal Resource Identifier* (URI). Examples of entities are people, objects, concepts, and dreams. Other nodes represent *literals* (raw data values), such as numbers, dates, and strings.\n",
    "\n",
    "Dealing with these different labels can be challenging. It is therefore common to simplify the knowledge graph to a regular digraph. This is done by first enumerating all unique labels, and by then re-encoding the graph using these numbers. This is done separately for the nodes and the relations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dc2e16a",
   "metadata": {},
   "source": [
    "### Task 1: Encode the graph using incides\n",
    "\n",
    "Write a procedure to transform the graph to an integer-encoding. For example, given a mapping `'maried' = 0`, `'age' = 5`, `'25' = 66`,`'john' = 43`, and `'kate' = 187`, the triples `(john, married, kate)` and `(john, age, 25)` should become `[43, 0, 187]` and `[43, 5, 66]`. Ensure that the encoding of the training set corresponds with that of the test set. The result should be two tensors and four maps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d693d41d-71c3-402d-a1e4-e2653cc845ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate mapping tables\n",
    "\n",
    "i2n = set()  # integers to nodes map\n",
    "i2r = set()  # integers to relation map\n",
    "\n",
    "# your code here\n",
    "# r2i = ...\n",
    "# n2i = ...\n",
    "# ...\n",
    "\n",
    "num_nodes = len(i2n)\n",
    "num_relations = len(i2r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35fb34fd-a020-4839-ac13-c0cf5f8a6299",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert the graphs to an integer-encoding tensor\n",
    "\n",
    "data_train = torch.zeros((len(g_train), 3), dtype=int)\n",
    "# ...\n",
    "\n",
    "data_test = "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40f40fba-32e9-40e4-bfc9-43888019bf34",
   "metadata": {},
   "source": [
    "Run the cell below to test your procedures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efdef813-a8e6-4253-9c6a-baa755d5683f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test your procedure.\n",
    "\n",
    "print('TRAIN:', end='\\n')\n",
    "for triple in data_train[:10]:\n",
    "    print(triple, end='\\n')\n",
    "\n",
    "print('TEST:', end='\\n')\n",
    "for triple in data_test[:10]:\n",
    "    print(triple, end='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62b03bdd-2e7e-4abd-9e59-1c93fc1f8665",
   "metadata": {},
   "source": [
    "## A simple triple scoring model\n",
    "\n",
    "Under the hood, a typical link prediction workflow defines a mathematical function f that takes a link as input and then returns the score of that link, or *triple*. The higher the score, the more likely the link is thought to exist. Given a set of candidate links, we can therefore compute all their scores and rank them in descending order. The top-k links are then the most likely to exist according to our model. Of course, to get reliable scores our model first needs to learn how to recognize possible valid links.\n",
    "\n",
    "There are various ways for a link prediction model to learn how to distinguish between valid and invalid links. Some models use a parameterised scoring function with a set of weights that it updates every epoch, while treating the links' vector representations as immutable objects that have been initialized before training, as is common with traditional machine learning. Other methods, such as the one you will implement today, approach the problem in a different way, by optimizing the links' vector representations themselves. In a sense, the vector representations act as their own weights. \n",
    "\n",
    "### DistMult\n",
    "\n",
    "Around 2015, a group of researchers of the Cornell University in the USA published a [paper](https://arxiv.org/abs/1412.6575) [1] in which they introduced a simple bilinear triple scoring function which performed surprisingly well on link prediction tasks. This scoring function, called *DistMult*, is defined as\n",
    "\n",
    "$$ y^T_{e1} M_r y_{e2} \\quad\\quad\\text{(formula 2 in [1])}$$\n",
    "\n",
    "with $y_{i}$ the internal representation belonging to node $i$, and with $M_r$ the diagonal matrix representation for relationship type $r$. Both these representations are updated each iteration by optimizing for a strong separation between positive and negative samples.\n",
    "\n",
    "    \n",
    "*[1] Yang, B., Yih, S. W. T., He, X., Gao, J., & Deng, L. (2015). Embedding Entities and Relations for Learning and Inference in Knowledge Bases. In Proceedings of the International Conference on Learning Representations (ICLR) 2015.*\n",
    "\n",
    "### Task 2: Implement DistMult\n",
    "\n",
    "Implement DistMult as a subclass of the PyTorch `nn.Module` class. Specifically, this entails writing the initialisation function `__init__()` and a forward function `forward()`. Make it possible to specify the size of the dimensions used for the various vector representations. Test your model afterwards on a small subset of the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b50a9a8-2903-494b-8c74-7922fde18ec9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DistMult(nn.Module):\n",
    "    def __init__(self, ...):\n",
    "        \"\"\" DistMult\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        # your code here\n",
    "\n",
    "    def forward(self, X:torch.Tensor):\n",
    "        # your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccd3307f-b77b-4137-a254-e65d9f6bb1e1",
   "metadata": {},
   "source": [
    "Run the following cell to initialize and test your implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0a775af-e91c-4ab3-9a66-6fe60c11b653",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DistMult(...)\n",
    "\n",
    "samples = data_train[:10]\n",
    "out = model(samples)\n",
    "\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a5b1aaa-230c-43c6-9689-7a82f0e14dd1",
   "metadata": {},
   "source": [
    "## Negative Sampling\n",
    "\n",
    "A common approach for training link prediction models is to present the model with a set of positive and negative samples. Here, the positive samples are links that are known to exist in the graph, whereras the negative samples are links that are known never to exist in the graph. In practice, the set of positive samples is often just a subset of the graph, while the set of negative samples is generated by corrupting the existing links using some strategy. This is called *negative sampling*. By labeling the positive and negative samples accordingly, and by updating the weights on how these samples are scored, link prediction models learn to rank candidate links accurately.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae08cf12-16ae-408d-9713-3af4d72afa7e",
   "metadata": {},
   "source": [
    "### Task 3a: Corrupting triples\n",
    "\n",
    "Write a procedure to generate negative samples. The procedure should return a new tensor with non-existing triples. Use Python comments to explain the strategy that you decided on, and test your procedure on a small subset of the data afterwards."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5bb694b-1728-4696-9b8e-eb93c5f2fe7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def corrupt_triples(...):\n",
    "    # your code here\n",
    "\n",
    "    return data_corrupt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "640ccfc8-6f4a-4fc9-911f-0635e3e5e193",
   "metadata": {},
   "source": [
    "Run the following cell to test your procedure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6abd3180-b720-4b5d-9d19-dbdfa6953c11",
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = data_train[:10]\n",
    "print(f\"original data:\\n{samples}\\n\")\n",
    "\n",
    "samples_corrupted = corrupt_triples(...)  # add your parameters\n",
    "print(f\"corrupted data:\\n{samples_corrupted}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89ae3e28",
   "metadata": {},
   "source": [
    "## Mini Batching\n",
    "\n",
    "Real-world graphs are often quite large, which translates in a large number of links. It is, for example, not uncommon for a graph to have many millions of links. Just think about how many people are subscribed to Facebook, how many others they are connected to, and how many attributes an average user has associated with their account. Learning over so many links in a single go is often impossible, especially if we want to speed up our computations using a GPU, which has a limited amount of on-device memory.\n",
    "\n",
    "Mini batching is a technique that enables you to learn on partitions of the data. This is typically achieved by splitting the data into roughly even parts, and by updating the model after each part. Next to alleviating scalibity issues, updating the model after seeing a subset of the data has the additional benefit of creating a smoother convergence, since any sudden changes in gradient by individual samples is compensated by other samples in the batch.\n",
    "\n",
    "### Task 3b: Creating batches\n",
    "\n",
    "Write a procedure to generate mini batches of triples. The procedure should return a list of batches. Take into account the overhead that copying batches to GPU has by distributing the triples in some sensible way. The preferred batch size must be a parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "907bcea9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mk_batches(...) -> list:\n",
    "    # your code here\n",
    "    return batches"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8915a8de",
   "metadata": {},
   "source": [
    "Run the following cell to test your function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf151ba6",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, batch in enumerate(mk_batches(...), 1):  # add your parameters\n",
    "    print(f\"batch {i}: {batch}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "683fb5d0",
   "metadata": {},
   "source": [
    "## Training and testing\n",
    "\n",
    "Training and testing are two important steps in the experimental workflow. Here, it is important to remember that the test data must be used exactly once, for testing, and that the test run must use a predetermined set of hyperparameter values. If this were not the case, then you are simply optimizing for improved performance on the test split rather than on the entire dataset. \n",
    "\n",
    "While left out here, it is also often good practice to create a validation set. The validation set can be used for hyperparameter optimization by training the model with various different hyperparameter values and by evaluating the performance on the validation set."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad95f107-2f27-47f2-a827-96337ab6aeda",
   "metadata": {},
   "source": [
    "### Task 4a: The training loop\n",
    "\n",
    "Write a procedure to train the model. Specifically, create a loop that passes the entire training set through the model every epoch, while computing the loss and updating the weights after each batch. Ensure that your batches and negative samples get randomized or regenerated each epoch. Use the Adam optimizer and the BCE with logits loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53423cec-086b-4110-a8b2-206e7f02c9fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set hyperparameters\n",
    "learning_rate = ...\n",
    "num_epoch = 50\n",
    "corrupt_probability = ...\n",
    "batch_size = ...\n",
    "\n",
    "# set optimizer and loss function\n",
    "optimizer = ...\n",
    "loss_function = ...\n",
    "\n",
    "num_train = data_train.shape[0]\n",
    "for epoch in range(1, num_epoch+1):\n",
    "    print(f'Epoch {epoch:3d} - ', end='')\n",
    "\n",
    "    # create batches\n",
    "    batches = ...         # your code here\n",
    "    num_batches = len(batches)\n",
    "\n",
    "    batch_loss_tensor = torch.zeros(num_batches, dtype=torch.float32)\n",
    "    for batch_id, batch in enumerate(batches):\n",
    "        data_batch = ...\n",
    "        num_samples = data_batch.shape[0]\n",
    "\n",
    "        # create negative samples by randomly assigning random node indices in the object position\n",
    "        data_batch_corrupt = ...\n",
    "        ...\n",
    " \n",
    "        # create labels; positive samples are 1, negative 0\n",
    "        Y = ... \n",
    "\n",
    "        # allow model parameters to be learned   \n",
    "        model.train()         \n",
    "\n",
    "        # compute scores for positive and negative triples  \n",
    "        Y_hat = ...\n",
    "        \n",
    "        # compute loss\n",
    "        batch_loss = ...\n",
    "        \n",
    "        # Zero gradients, perform a backward pass, and update the weights.\n",
    "        optimizer.zero_grad()\n",
    "        batch_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        batch_loss = float(batch_loss)  # release memory of computation graph\n",
    "        batch_loss_tensor[batch_id] = batch_loss\n",
    "\n",
    "    print(f'loss on train set: {batch_loss_tensor.mean():0.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cdc2246-8bd8-4d5b-aca1-89d2a0a65211",
   "metadata": {},
   "source": [
    "### Task 4b: The test loop\n",
    "\n",
    "Write a procedure to test the now-trained model. Specifically, create a loop that passes the entire test set through the model every epoch, while computing the loss after each batch. Ensure that the weights of your model are frozen during testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4156df0f-3662-471c-90e8-1eb30d9d24e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_test = data_test.shape[0]\n",
    "\n",
    "batch_size = ...\n",
    "batches = ...\n",
    "num_batches = len(batches)\n",
    "\n",
    "batch_loss_tensor = torch.zeros(num_batches, dtype=torch.float32)\n",
    "for batch_id, batch in enumerate(batches):\n",
    "    data_batch = ...\n",
    "    num_samples = data_batch.shape[0]\n",
    "\n",
    "    # create negative samples by randomly assigning random node indices in the object position\n",
    "    data_batch_corrupt = ...\n",
    "\n",
    "    # create labels; positive samples are 1, negative 0\n",
    "    Y = ...\n",
    "\n",
    "    # freeze model parameters for evaluation\n",
    "    model.eval()\n",
    "    \n",
    "    # compute scores for positive and negative triples  \n",
    "    Y_hat = ...\n",
    "\n",
    "    # compute loss\n",
    "    batch_loss = ...\n",
    "\n",
    "    batch_loss = float(batch_loss)  # release memory of computation graph\n",
    "    batch_loss_tensor[batch_id] = batch_loss\n",
    "\n",
    "    # compute hits@k (see task 5)\n",
    "    hits_at_k = ...\n",
    "\n",
    "print(f'loss on test set: {batch_loss_tensor.mean():0.4f}')\n",
    "for k,v in hits_at_k.items():\n",
    "    print(f'hits@{k}: {v:0.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cfe37bf-43db-4b2f-87c5-fcee9068c013",
   "metadata": {},
   "source": [
    "### Evaluation\n",
    "\n",
    "Two different metrics are commonly used to evaluate link prediction models: the *mean reciprocal rank* (MRR) and *hits@k*. The MRR equals the arithmetic mean of reciprocal ranks, and is defined as $$ MRR = \\frac{1}{|\\mathcal{T}|} \\displaystyle\\sum_{t \\in \\mathcal{T}} r(t)^{-1}$$\n",
    "\n",
    "with $\\mathcal{T}$ the set of positive triples and $r(t)$ the rank of triple $t$. The MRR returns a score in $(0, 1]$ where higher is better.\n",
    "\n",
    "The *hits@k* equals the fraction of positive samples that appear in the first $k$ entries of the sorted rank list. The metric is similar to the MRR, yet cheaper to compute and more sensitive to irregularities. The *hits@k* is formaly defined as $$\\text{hits@k} = \\frac{1}{|\\mathcal{T}|} \\displaystyle\\sum_{t \\in \\mathcal{T}} 1 \\text{ iff } r(t) \\le k$$\n",
    "\n",
    "Common values for $k$ are 1, 3, 5, and 10. The returned score lie between $(0, 1]$ where higher is better."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ba89c1f-2ada-4356-bbaf-5f0212a87481",
   "metadata": {},
   "source": [
    "### Task 5: Evaluate the model\n",
    "\n",
    "Write a procedure to calculate the *hits@k* metric and update task 4b to output the average *hits@k* score for $k \\in \\{1, 3, 10\\}$. Ensure that the score equals $1.0$ if all top-$k$ entries are positive samples. Be aware that you will need to rerun the test loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aad0c36e-98e6-4bf8-b2ac-4a115749f1d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_hits_at_k(...) -> float:\n",
    "     # your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc3587d5-5814-4bac-93ce-7f3aafbf099c",
   "metadata": {},
   "source": [
    "## Deliverable\n",
    "\n",
    "Once you and your team member are satisfied with the results, you are to rerun the notebook from scratch and to export it to PDF format. This cleans up the output and makes it easier for us to provide feedback. Specifically, first select Restart kernel and run all cells followed by Save and export Notebook as -> PDF.\n",
    "\n",
    "**Please rename the PDF file to `ML4G-PA1_GROUP<groupID>.pdf` and submit the file before noon the next Monday**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml4graphs",
   "language": "python",
   "name": "ml4graphs"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
